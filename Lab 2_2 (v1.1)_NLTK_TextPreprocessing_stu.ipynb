{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISSS609 : Text Analytics and Applications\n",
    "\n",
    "## Lab 2: Natural Language Toolkit (NLTK)\n",
    "\n",
    "### Objectives \n",
    "\n",
    "-   To be able to load your own text collections using NLTK.\n",
    "\n",
    "-   To be able to perform basic text preprocessing operations (tokenization, stemming and stop word removal)\n",
    "    using Python and NLTK."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "<p>Many modern text analytics tools automate the pre-processing of text documents. In order to understand and customise modern tools, it is important to recognise common pre-processing steps.</p>\n",
    "<p>The tools afforded by NLTK is our entry point.</p>\n",
    "<p>NLTK is a library already included with the default Anaconda installation. If for some reason, you did not have NLTK installed, you can refer to <a href='https://www.nltk.org/install.html'>https://www.nltk.org/install.html</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Your Own Text\n",
    "\n",
    "Although NLTK provides a number of interesting document collections, we always need to analyze our own text collections. The first part of this lab is to show you how to load your own text collections into NLTK.\n",
    "\n",
    "We typically refer to a digitalized collection of documents as a *corpus*. A corpus contains a set of documents. While many real world documents are in the format of Microsoft Word or PDF, when we process and analyze documents, they are usually converted into plain text files. Here we assume that the corpus we plan to analyze contains only plain text files.\n",
    "\n",
    "First, create one `.txt` file for each document in your collection. For example, if you have 100 documents, you can name them `1.txt`, `2.txt`, $\\ldots$, `100.txt`. You are free to choose any document name as long as each document has a unique name. Place all the `.txt` files in a directory of your choice. Next, you would like to load these files using the NLTK package such that you can then directly work with the words inside these files. In the NLTK package, there are existing tools that can be used to read in textual files. Let us start with a simple one called <b> `PlaintextCorpusReader` </b> to deal with plain text files that do not have annotations such as HTML tags.\n",
    "\n",
    "The following code shows how you can load plain text files using NLTK. In the example below, it is assume that there are two files named `haze.txt` and `mrt.txt` inside the directory `data/SampleText`, where `data` should be placed in the current directory, i.e., where this Jupyter notebook is placed. (Note that the `data` directory with the two text files inside should have been downloaded together with this Jupyter notebook.) If you have placed the data in a different directory, you can modify the code below to correspond to the correct directory where your files are.\n",
    "We also encourage you to create different folders for different labs to avoid confusion.\n",
    "\n",
    "For those of you new to Python, the lines starting with `#` are *comments*, which explain what the code does but is not executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following statement imports the NLTK package.\n",
    "import nltk\n",
    "# The following statement imports a class called PlaintextCorpusReader.\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    " \n",
    "# This variable specifies the directory where the text files are.\n",
    "file_directory = 'data/SampleText'\n",
    "\n",
    "# This variable is a regular expression that specifies the pattern of the filenames we consider. \n",
    "# Here this regular expression mathces only those filenames ending with '.txt'.\n",
    "filename_pattern = '.+\\.txt'\n",
    "\n",
    "# We can now create a PlaintextCorpusReader object with the file directory and filename pattern defined above.\n",
    "my_corpus = PlaintextCorpusReader(file_directory, filename_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "- For those of you who are familiar with Python programming and would like to learn more about the code above, you can refer to the following URL for the documentation of the `PlaintextCorpusReader` class: http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.plaintext.PlaintextCorpusReader\n",
    "- You can also find the source code of this class on the following page: http://www.nltk.org/_modules/nltk/corpus/reader/plaintext.html#PlaintextCorpusReader\n",
    "- For Python regular expressions, you can refer to the link below: https://docs.python.org/3/library/re.html\n",
    "- Recommended online resource for Python documentation is https://kite.com/python/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the three documents `gates.txt`, `haze.txt` and `mrt.txt` have been read into `my_corpus`. We can use the code below to verify this. Here `my_corpus.fileids()` shows the IDs of all the documents stored inside `my_corpus`. You can see that the IDs of the documents are simply the names of the files in the directory that has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gates.txt', 'haze.txt', 'mrt.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to class the functions in this class, PlaintextCorpusReader?\n",
    "\n",
    "`my_corpus.words('haze.txt')` returns all the words inside the document `haze.txt`, which are stored into the variable `haze`. Using `haze[0:30]`, we show the first 30 words in `haze`. You can open the file `haze.txt` directly to verify that these are indeed the first 30 words in the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Singapore', 'can', 'expect', 'more', 'rain', 'and', 'less', 'haze', 'in', 'the', 'coming', 'weeks', 'with', 'the', 'south', '-', 'west', 'monsoon', 'season', 'transitioning', 'into', 'inter', '-', 'monsoon', 'conditions', '.', 'The', 'inter', '-', 'monsoon']\n"
     ]
    }
   ],
   "source": [
    "haze = my_corpus.words('haze.txt')\n",
    "print(haze[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use a function called `len` to find out the length, i.e., the total number of words, insize this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "print(len(haze))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `FreqDist()` provided by NLTK.\n",
    "`FreqDist()` can be applied to any list in Python. We can now use\n",
    "`FreqDist()` on our own text as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 11), ('and', 7), ('in', 5), ('.', 4), ('Singapore', 3), ('haze', 3), ('-', 3), ('monsoon', 3), ('season', 3), ('of', 3)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(haze)\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the code above displays the most frequent 10 words\n",
    "inside the document `haze.txt`.\n",
    "\n",
    "What if you would like to get the words from *all* the files in\n",
    "`my_corpus`? You can simply use `my_corpus.words()` without specifying any document ID. Give it a try. Can you find out how many words there are in total in both `haze.txt` and `mrt.txt`? Can you find out the most frequent 10 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613\n",
      "[('.', 31), ('the', 30), (',', 28), ('and', 26), ('of', 18), ('to', 17), ('in', 11), ('Gates', 9), ('-', 9), ('Mr', 8)]\n"
     ]
    }
   ],
   "source": [
    "# enter your code here\n",
    "wholeCorpus=my_corpus.words()\n",
    "print(len(wholeCorpus))\n",
    "print(nltk.FreqDist(wholeCorpus).most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "This data set in the `SGNews_Apr2012` folder contains a set of Singapore news articles in April 2012. Load this document collection using NLTK. Can you find out the following information of this collection?\n",
    "\n",
    "-   Number of documents in the collection.\n",
    "-   Total number of words in the collection.\n",
    "-   The top-20 most frequent words in the file, `14011.txt`.\n",
    "\n",
    "### Tips:\n",
    "\n",
    "-   To use `FreqDist`, you can either use `nltk.FreqDist()` as shown above or type `from nltk.probability import FreqDist` first and then directly use\n",
    "    `FreqDist()`. This is because the `FreqDist` class is defined by the `probability` module under NLTK.\n",
    "\n",
    "-   You may wonder how to find out the number of unique words. You can first define a `FreqDist` object from all the words and then find out the length of the\n",
    "    `FreqDist` object.\n",
    "    \n",
    "- You may google for the ideas on the unique words.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "111485\n",
      "[(',', 27), ('.', 24), ('he', 12), (':', 10), ('to', 10), (\"'\", 9), ('at', 9), ('George', 9), ('\"', 9), ('a', 8), ('-', 8), ('in', 8), ('and', 8), ('of', 7), ('t', 7), ('1', 6), ('IQ', 6), ('is', 6), ('the', 6), ('his', 6)]\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here to answer the questions above. \n",
    "\n",
    "# The following statement imports a class called PlaintextCorpusReader (not needed if already imported)\n",
    "\n",
    "\n",
    "# Define the directory variable \n",
    "file_directory = \"data/SGNews_Apr2012/\"\n",
    "\n",
    "# Define the file patterns\n",
    "filename_pattern = \".+\\.txt\"\n",
    "\n",
    "# Define the corpus variable using PlaintextCorpusReader with the directory and file patterns\n",
    "newsCorpus = PlaintextCorpusReader(file_directory,filename_pattern)\n",
    "\n",
    "# Print the number of files (use fileids)\n",
    "print(len(newsCorpus.fileids()))\n",
    "\n",
    "# Print total number of words in corpus\n",
    "print(len(newsCorpus.words()))\n",
    "\n",
    "# Define a variable (file14011) with words from specific file \n",
    "file14011 = newsCorpus.words(\"14011.txt\")\n",
    "\n",
    "# Define the freq dist on that file words. Call it file14011Freq\n",
    "file14011Freq = nltk.FreqDist(file14011)\n",
    "\n",
    "# Print top 20 words in file14011Freq\n",
    "print(file14011Freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "You must have noticed that by using the `PlaintextCorpusReader`, tokenization is done while loading the files, that is, the original text is split into individual words (more formally, *tokens*) and stored as a list of tokens in Python.\n",
    "\n",
    "### Sentence Splitting \n",
    "\n",
    "What if you would like to split the text into sentences? For some text analysis such as extractive text summarization, it is useful to look at individual sentences. Actually this has also been done by the `PlaintextCorpusReader`. Take a look at the code below. Here `my_corpus.sents('haze.txt')` returns not the list of words inside `haze.txt` but the list of sentences inside `haze.txt`, where each sentence is itself a list of words. For example, `haze_sents[0]` is the first sentence in `haze.txt`, and this sentence is represented as a list of words, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Singapore', 'can', 'expect', 'more', 'rain', 'and', 'less', 'haze', 'in', 'the', 'coming', 'weeks', 'with', 'the', 'south', '-', 'west', 'monsoon', 'season', 'transitioning', 'into', 'inter', '-', 'monsoon', 'conditions', '.']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# From my_corpus, get the sentences in the document haze.txt\n",
    "haze_sents = my_corpus.sents('haze.txt')\n",
    "\n",
    "# Inspect the first sentence\n",
    "try:\n",
    "    print(haze_sents[0])\n",
    "except LookupError:\n",
    "    # if the punkt library is not downloaded, this segment will do so and then try again\n",
    "    nltk.download('punkt')\n",
    "    print(haze_sents[0])\n",
    "\n",
    "# Display the number of sentences\n",
    "print(len(haze_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find out how many sentences are there in `haze.txt` and in `mrt.txt`, respectively?\n",
    "\n",
    "After loading your own collection of text, you may want to perform some simple text pre-processing steps to further clean or normalize the text so that it is easier to use the text in later analysis.\n",
    "\n",
    "### Changing Everything to Lowercase \n",
    "\n",
    "In English, sentences start with capitalized words. However, for many text analysis tasks, we should not differentiate between a capitalized word and its original form. For example, the word “Yesterday” and “yesterday” in the following two sentences carry the same meaning and should be represented in the same form.\n",
    "\n",
    "> *Yesterday I went to a party.*\n",
    ">\n",
    "> *My friend Sally left for New York yesterday.*\n",
    "\n",
    "A simple solution is to change all letters into lowercase when tokenizing text. Python provides a simple function called `lower()` that does the job.\n",
    "\n",
    "We can see below that the words “Singapore” and “The” in the original list `haze` has been changed to “singapore” and\n",
    "“the” in the transformed list `haze_lower`. Here `[w.lower() for w in haze]` defines a new list by taking every word in\n",
    "the list `haze` and changing it to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Singapore', 'can', 'expect', 'more', 'rain', 'and', 'less', 'haze', 'in', 'the', 'coming', 'weeks', 'with', 'the', 'south', '-', 'west', 'monsoon', 'season', 'transitioning', 'into', 'inter', '-', 'monsoon', 'conditions', '.', 'The', 'inter', '-', 'monsoon']\n"
     ]
    }
   ],
   "source": [
    "# The words of the document haze.txt is now in the variable haze\n",
    "haze = my_corpus.words('haze.txt')\n",
    "\n",
    "# Inspect them\n",
    "print(haze[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['singapore', 'can', 'expect', 'more', 'rain', 'and', 'less', 'haze', 'in', 'the', 'coming', 'weeks', 'with', 'the', 'south', '-', 'west', 'monsoon', 'season', 'transitioning', 'into', 'inter', '-', 'monsoon', 'conditions', '.', 'the', 'inter', '-', 'monsoon']\n"
     ]
    }
   ],
   "source": [
    "# Implied for loop using list comprehension to convert words to lowercase\n",
    "haze_lower = [w.lower() for w in haze]\n",
    "\n",
    "# We should see the same words as above, but in lowercase\n",
    "print(haze_lower[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation Marks \n",
    "\n",
    "You will also notice that after tokenization, all punctuation marks are kept inside the list of words. For many text analysis tasks such as text classification and document retrieval, punctuation marks are not useful. (For some other tasks such as information extraction, punctuation marks can be very useful.)\n",
    "\n",
    "If we would like to remove punctuation marks and other non-word tokens such as numbers, what shall we do? One solution is to first list down all the special tokens such as comma and period that we want to remove, and then remove all the tokens in our document collection that are in this list. However, we may not be able to enumerate all possible tokens that need to be removed. Another solution is to simply keep tokens that contain only alphabetic characters (letters) or alphanumerical characters (letters and numbers), depending on the need of the analysis task.\n",
    "\n",
    "To achieve this goal, people often use something called *regular expressions*. Using regular expressions, you can define a wide range of patterns to match strings. Many programming languages have support for regular expressions. In Python, you need to import the `re` library before you can use regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['singapore', 'can', 'expect', 'more', 'rain', 'and', 'less', 'haze', 'in', 'the', 'coming', 'weeks', 'with', 'the', 'south', 'west', 'monsoon', 'season', 'transitioning', 'into', 'inter', 'monsoon', 'conditions', 'the', 'inter', 'monsoon', 'season', 'typically', 'lasts', 'from']\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "haze_words_only = [w for w in haze_lower if re.search('^[a-z]+$', w)]\n",
    "print(haze_words_only[0:30])   \n",
    "print(len(haze_words_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['singapore', 'can', 'expect', 'more', 'rain', 'and', 'less', 'haze', 'in', 'the', 'coming', 'weeks', 'with', 'the', 'south', 'west', 'monsoon', 'season', 'transitioning', 'into', 'inter', 'monsoon', 'conditions', 'the', 'inter', 'monsoon', 'season', 'typically', 'lasts', 'from']\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "# see if you can achieve the same effect using built-in Python string function\n",
    "haze_words_only = [w for w in haze_lower if w.isalpha()]\n",
    "print(haze_words_only[0:30])\n",
    "print(len(haze_words_only))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Using the `SGNews_Apr2012` corpus that you have loaded,  and the file `14011.txt`\n",
    "1. First change every token to lower case. \n",
    "2. Use regular expressions to keep only alphabetic tokens. \n",
    "3. Find out the most frequent tokens after these steps.\n",
    "\n",
    "Compare them with the most frequent tokens you have found earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from 14011.txt in lowercase:\n",
      " ['htmlid', ':', '14011', 'title', ':', 'he', \"'\", 's', 'a', 'mensa', 'member', 'at', '7', 'url', ':', 'http', '://', 'www', '.', 'asiaone', '.', 'com', '/', 'news', '/', 'latest', '%', '2bnews', '/', 'singapore', '/', 'story', '/', 'a1story20120430', '-', '342911', '.', 'html', 'numofpages', ':', '1', 'content', ':', 'this', 'primary', '1', 'boy', 'does', 'mathematics', 'problems', 'meant', 'for', 'secondary', '1', 'students', 'without', 'batting', 'an', 'eyelid', '.', 'at', 'just', 'seven', 'years', 'old', ',', 'george', 'yeo', ',', 'with', 'an', 'iq', 'of', '130', ',', 'is', 'one', 'of', 'the', 'youngest', 'members', 'in', 'mensa', 'singapore', '.', 'the', 'high', '-', 'iq', 'society', 'members', 'are', 'mostly', 'between', 'the', 'ages', 'of', '18', 'and', '35', '.', 'the', 'average', 'iq', '-', 'intelligence', 'quotient', '-', 'is', '100', '.', 'when', 'asked', 'why', 'he', 'joined', 'mensa', ',', 'george', 'just', 'blinks', 'owlishly', 'from', 'behind', 'his', 'glasses', 'and', 'says', 'in', 'a', 'matter', '-', 'of', '-', 'fact', 'tone', ':', '\"', 'because', 'i', 'have', 'a', 'high', 'iq', '.\"', 'his', 'iq', 'exceeds', 'about', '98', 'per', 'cent', 'of', 'children', 'his', 'age', '.', 'george', 'joined', 'the', 'organisation', 'in', 'the', 'middle', 'of', 'last', 'year', ',', 'when', 'he', 'was', 'six', 'years', 'old', '.', 'his', 'father', ',', 'mr', 'yeo', 'kee', 'lin', ',', '46', ',', 'explains', ':', '\"', 'we', 'were', 'actually', 'holding', 'back', 'at', 'first', ',', 'because', 'there', 'was', 'no', 'purpose', 'or', 'advantage', '.', '\"', 'but', 'last', 'year', ',', 'his', 'k2', 'teacher', 'told', 'us', 'george', 'wasn', \"'\", 't', 'paying', 'attention', 'in', 'class', '.', 'he', 'already', 'knew', 'everything', 'they', 'were', 'being', 'taught', ',', 'so', 'he', 'was', 'getting', 'bored', '.\"', 'mr', 'yeo', 'says', 'that', 'at', '10', 'months', ',', 'george', 'could', 'already', 'recognise', 'letters', 'and', 'at', '21', 'months', ',', 'he', 'could', 'read', 'from', 'a', '-', 'z', 'and', 'count', 'up', 'to', '20', ',', 'and', 'in', 'tens', 'after', 'that', '.', 'in', 'nursery', '1', ',', 'at', 'three', 'years', 'old', ',', 'he', 'was', 'already', 'reading', 'while', 'other', 'children', 'were', 'still', 'struggling', 'to', 'learn', 'their', 'letters', 'and', 'numbers', '.', 'when', 'george', 'was', 'four', ',', 'he', 'told', 'his', 'parents', 'he', 'didn', \"'\", 't', 'want', 'to', 'go', 'to', 'school', 'any', 'more', '.', '\"', 'don', \"'\", 't', 'waste', 'your', 'money', ',\"', 'he', 'told', 'them', '.', '\"', 'i', 'already', 'know', 'everything', '.\"', 'his', 'parents', 'kept', 'him', 'in', 'school', 'to', 'build', 'his', 'social', 'skills', ',', 'but', 'got', 'him', 'accelerated', 'learning', 'worksheets', 'to', 'challenge', 'him', 'with', 'at', 'home', '.', 'even', 'though', 'this', 'boy', 'wonder', 'can', \"'\", 't', 'skip', 'grades', 'easily', 'in', 'the', 'school', 'system', ',', 'his', 'parents', 'don', \"'\", 't', 'mind', '.', '\"', 'we', 'want', 'him', 'to', 'play', 'more', ',', 'and', 'to', 'learn', 'teamwork', '.', 'school', 'is', 'important', 'for', 'building', 'character', ',', 'not', 'just', 'knowledge', ',\"', 'says', 'mr', 'yeo', ',', 'a', 'company', 'director', '.', 'his', 'wife', 'madam', 'maria', 'yan', ',', '44', ',', 'is', 'a', 'banking', 'officer', '.', 'george', 'is', 'their', 'only', 'child', '.', 'the', 'anglo', '-', 'chinese', 'school', '(', 'junior', ')', 'student', 'juggles', 'fractions', 'confidently', 'and', 'he', 'loves', 'algebra', 'most', '\"', 'because', 'they', 'try', 'to', 'use', 'letters', 'to', 'trick', 'us', '.', 'but', 'they', 'can', \"'\", 't', 'trick', 'me', '.\"', 'george', 'is', 'apparently', 'also', 'quick', 'with', 'the', 'ladies', '.', 'says', 'mr', 'yeo', 'with', 'a', 'laugh', ':', '\"', 'even', 'when', 'he', 'was', 'two', ',', 'whenever', 'we', 'got', 'into', 'a', 'lift', 'with', 'a', 'young', 'lady', ',', 'he', 'would', 'make', 'conversation', '.', 'when', 'it', \"'\", 's', 'an', 'old', 'aunty', ',', 'he', 'won', \"'\", 't', 'even', 'look', 'at', 'her', '.\"', 'george', 'tells', 'this', 'reporter', 'cheekily', ':', '\"', 'let', 'me', 'tell', 'you', 'something', ':', 'saranghaeyo', '(', 'i', 'love', 'you', 'in', 'korean', ').\"', 'iq', 'of', '130', 'recognised', 'alphabet', 'at', '10', 'months', 'does', 'sec', '1', 'maths', 'at', 'pri', '1'] \n",
      "\n",
      "Words from 14011.txt (no puctuations):\n",
      " ['htmlid', 'title', 'he', 's', 'a', 'mensa', 'member', 'at', 'url', 'http', 'www', 'asiaone', 'com', 'news', 'latest', 'singapore', 'story', 'html', 'numofpages', 'content', 'this', 'primary', 'boy', 'does', 'mathematics', 'problems', 'meant', 'for', 'secondary', 'students', 'without', 'batting', 'an', 'eyelid', 'at', 'just', 'seven', 'years', 'old', 'george', 'yeo', 'with', 'an', 'iq', 'of', 'is', 'one', 'of', 'the', 'youngest', 'members', 'in', 'mensa', 'singapore', 'the', 'high', 'iq', 'society', 'members', 'are', 'mostly', 'between', 'the', 'ages', 'of', 'and', 'the', 'average', 'iq', 'intelligence', 'quotient', 'is', 'when', 'asked', 'why', 'he', 'joined', 'mensa', 'george', 'just', 'blinks', 'owlishly', 'from', 'behind', 'his', 'glasses', 'and', 'says', 'in', 'a', 'matter', 'of', 'fact', 'tone', 'because', 'i', 'have', 'a', 'high', 'iq', 'his', 'iq', 'exceeds', 'about', 'per', 'cent', 'of', 'children', 'his', 'age', 'george', 'joined', 'the', 'organisation', 'in', 'the', 'middle', 'of', 'last', 'year', 'when', 'he', 'was', 'six', 'years', 'old', 'his', 'father', 'mr', 'yeo', 'kee', 'lin', 'explains', 'we', 'were', 'actually', 'holding', 'back', 'at', 'first', 'because', 'there', 'was', 'no', 'purpose', 'or', 'advantage', 'but', 'last', 'year', 'his', 'teacher', 'told', 'us', 'george', 'wasn', 't', 'paying', 'attention', 'in', 'class', 'he', 'already', 'knew', 'everything', 'they', 'were', 'being', 'taught', 'so', 'he', 'was', 'getting', 'bored', 'mr', 'yeo', 'says', 'that', 'at', 'months', 'george', 'could', 'already', 'recognise', 'letters', 'and', 'at', 'months', 'he', 'could', 'read', 'from', 'a', 'z', 'and', 'count', 'up', 'to', 'and', 'in', 'tens', 'after', 'that', 'in', 'nursery', 'at', 'three', 'years', 'old', 'he', 'was', 'already', 'reading', 'while', 'other', 'children', 'were', 'still', 'struggling', 'to', 'learn', 'their', 'letters', 'and', 'numbers', 'when', 'george', 'was', 'four', 'he', 'told', 'his', 'parents', 'he', 'didn', 't', 'want', 'to', 'go', 'to', 'school', 'any', 'more', 'don', 't', 'waste', 'your', 'money', 'he', 'told', 'them', 'i', 'already', 'know', 'everything', 'his', 'parents', 'kept', 'him', 'in', 'school', 'to', 'build', 'his', 'social', 'skills', 'but', 'got', 'him', 'accelerated', 'learning', 'worksheets', 'to', 'challenge', 'him', 'with', 'at', 'home', 'even', 'though', 'this', 'boy', 'wonder', 'can', 't', 'skip', 'grades', 'easily', 'in', 'the', 'school', 'system', 'his', 'parents', 'don', 't', 'mind', 'we', 'want', 'him', 'to', 'play', 'more', 'and', 'to', 'learn', 'teamwork', 'school', 'is', 'important', 'for', 'building', 'character', 'not', 'just', 'knowledge', 'says', 'mr', 'yeo', 'a', 'company', 'director', 'his', 'wife', 'madam', 'maria', 'yan', 'is', 'a', 'banking', 'officer', 'george', 'is', 'their', 'only', 'child', 'the', 'anglo', 'chinese', 'school', 'junior', 'student', 'juggles', 'fractions', 'confidently', 'and', 'he', 'loves', 'algebra', 'most', 'because', 'they', 'try', 'to', 'use', 'letters', 'to', 'trick', 'us', 'but', 'they', 'can', 't', 'trick', 'me', 'george', 'is', 'apparently', 'also', 'quick', 'with', 'the', 'ladies', 'says', 'mr', 'yeo', 'with', 'a', 'laugh', 'even', 'when', 'he', 'was', 'two', 'whenever', 'we', 'got', 'into', 'a', 'lift', 'with', 'a', 'young', 'lady', 'he', 'would', 'make', 'conversation', 'when', 'it', 's', 'an', 'old', 'aunty', 'he', 'won', 't', 'even', 'look', 'at', 'her', 'george', 'tells', 'this', 'reporter', 'cheekily', 'let', 'me', 'tell', 'you', 'something', 'saranghaeyo', 'i', 'love', 'you', 'in', 'korean', 'iq', 'of', 'recognised', 'alphabet', 'at', 'months', 'does', 'sec', 'maths', 'at', 'pri'] \n",
      "\n",
      "[('the', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here to answer the questions above.\n",
    "# Use file14011 from the previous code, which was the list of words from 14011.txt\n",
    "\n",
    "# 1. First change every token to lower case.\n",
    "fileLower = [w.lower() for w in newsCorpus.words(\"14011.txt\")]\n",
    "print(\"Words from 14011.txt in lowercase:\\n\",fileLower, \"\\n\")\n",
    "\n",
    "# 2.Use regular expressions to keep only alphabetic tokens.\n",
    "fileLowerWordsOnly = [w for w in fileLower if w.isalpha()]\n",
    "print(\"Words from 14011.txt (no puctuations):\\n\", fileLowerWordsOnly, \"\\n\")\n",
    "\n",
    "# 3. Find out the most frequent tokens after these steps (fdist and FreqDist)\n",
    "fdist1 = nltk.FreqDist(fileLowerWordsOnly)\n",
    "print(fdist.most_common(1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: What if you want to keep all alphanumerical tokens, that is, you do not mind keeping tokens that contain digits? Try replacing `[a-z]` with `[a-z0-9]` and repeat the steps above.\n",
    "\n",
    "But do this after you have completed the lab exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal\n",
    "\n",
    "NLTK also has a built-in stop word list for English that can come in\n",
    "handy when we need to remove stop words from a text collection. The\n",
    "following code shows how we remove all the stop words from the list\n",
    "`haze_words_only`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['singapore', 'expect', 'rain', 'less', 'haze', 'coming', 'weeks', 'south', 'west', 'monsoon', 'season', 'transitioning', 'inter', 'monsoon', 'conditions', 'inter', 'monsoon', 'season', 'typically', 'lasts', 'october', 'november', 'weather', 'period', 'characterised', 'rainfall', 'light', 'variable', 'winds', 'meteorological']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop words are laguage-specific\n",
    "try:\n",
    "    stop_list = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stop_list = stopwords.words('english')\n",
    "\n",
    "# Remove the word w in haze_words_only if it is in the stop_list\n",
    "haze_stopremoved = [w for w in haze_words_only if w not in stop_list]\n",
    "\n",
    "# Inspect our handiwork\n",
    "print(haze_stopremoved[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can see that stop words such as 'can' and 'in' have been removed.</p>\n",
    "<p>Instead of using the built-in stop word list, you can also use your own stop word list if necessary.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('singapore', 3), ('haze', 3), ('monsoon', 3), ('season', 3), ('inter', 2), ('rainfall', 2), ('expect', 1), ('rain', 1), ('less', 1), ('coming', 1)]\n"
     ]
    }
   ],
   "source": [
    "# haze_stopremoved is a list of words (tokens).\n",
    "# to get the frequency of each token, we can use the built-in collections library\n",
    "from collections import Counter\n",
    "\n",
    "word_freq2 = Counter(haze_stopremoved)\n",
    "\n",
    "print(word_freq2.most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "1. Remove the stop words from `SGNews_Apr2012` file, `14011.txt` using NLTK’s built-in stop word list.\n",
    "\n",
    "2. Afterwards, find the most frequent 20 words in this file and see whether they give a good summary of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['htmlID', ':', '14011', 'Title', ':', 'He', \"'\", 'Mensa', 'member', '7', 'URL', ':', 'http', '://', 'www', '.', 'asiaone', '.', 'com', '/', 'News', '/', 'Latest', '%', '2BNews', '/', 'Singapore', '/', 'Story', '/', 'A1Story20120430', '-', '342911', '.', 'html', 'NumOfPages', ':', '1', 'Content', ':', 'This', 'Primary', '1', 'boy', 'mathematics', 'problems', 'meant', 'Secondary', '1', 'students', 'without', 'batting', 'eyelid', '.', 'At', 'seven', 'years', 'old', ',', 'George', 'Yeo', ',', 'IQ', '130', ',', 'one', 'youngest', 'members', 'Mensa', 'Singapore', '.', 'The', 'high', '-', 'IQ', 'society', 'members', 'mostly', 'ages', '18', '35', '.', 'The', 'average', 'IQ', '-', 'intelligence', 'quotient', '-', '100', '.', 'When', 'asked', 'joined', 'Mensa', ',', 'George', 'blinks', 'owlishly', 'behind', 'glasses', 'says', 'matter', '-', '-', 'fact', 'tone', ':', '\"', 'Because', 'I', 'high', 'IQ', '.\"', 'His', 'IQ', 'exceeds', '98', 'per', 'cent', 'children', 'age', '.', 'George', 'joined', 'organisation', 'middle', 'last', 'year', ',', 'six', 'years', 'old', '.', 'His', 'father', ',', 'Mr', 'Yeo', 'Kee', 'Lin', ',', '46', ',', 'explains', ':', '\"', 'We', 'actually', 'holding', 'back', 'first', ',', 'purpose', 'advantage', '.', '\"', 'But', 'last', 'year', ',', 'K2', 'teacher', 'told', 'us', 'George', \"'\", 'paying', 'attention', 'class', '.', 'He', 'already', 'knew', 'everything', 'taught', ',', 'getting', 'bored', '.\"', 'Mr', 'Yeo', 'says', '10', 'months', ',', 'George', 'could', 'already', 'recognise', 'letters', '21', 'months', ',', 'could', 'read', 'A', '-', 'Z', 'count', '20', ',', 'tens', '.', 'In', 'Nursery', '1', ',', 'three', 'years', 'old', ',', 'already', 'reading', 'children', 'still', 'struggling', 'learn', 'letters', 'numbers', '.', 'When', 'George', 'four', ',', 'told', 'parents', \"'\", 'want', 'go', 'school', '.', '\"', 'Don', \"'\", 'waste', 'money', ',\"', 'told', '.', '\"', 'I', 'already', 'know', 'everything', '.\"', 'His', 'parents', 'kept', 'school', 'build', 'social', 'skills', ',', 'got', 'accelerated', 'learning', 'worksheets', 'challenge', 'home', '.', 'Even', 'though', 'boy', 'wonder', \"'\", 'skip', 'grades', 'easily', 'school', 'system', ',', 'parents', \"'\", 'mind', '.', '\"', 'We', 'want', 'play', ',', 'learn', 'teamwork', '.', 'School', 'important', 'building', 'character', ',', 'knowledge', ',\"', 'says', 'Mr', 'Yeo', ',', 'company', 'director', '.', 'His', 'wife', 'Madam', 'Maria', 'Yan', ',', '44', ',', 'banking', 'officer', '.', 'George', 'child', '.', 'The', 'Anglo', '-', 'Chinese', 'School', '(', 'Junior', ')', 'student', 'juggles', 'fractions', 'confidently', 'loves', 'algebra', '\"', 'try', 'use', 'letters', 'trick', 'us', '.', 'But', \"'\", 'trick', '.\"', 'George', 'apparently', 'also', 'quick', 'ladies', '.', 'Says', 'Mr', 'Yeo', 'laugh', ':', '\"', 'Even', 'two', ',', 'whenever', 'got', 'lift', 'young', 'lady', ',', 'would', 'make', 'conversation', '.', 'When', \"'\", 'old', 'aunty', ',', \"'\", 'even', 'look', '.\"', 'George', 'tells', 'reporter', 'cheekily', ':', '\"', 'Let', 'tell', 'something', ':', 'saranghaeyo', '(', 'I', 'love', 'Korean', ').\"', 'IQ', '130', 'Recognised', 'alphabet', '10', 'months', 'Does', 'Sec', '1', 'maths', 'Pri', '1']\n",
      "20 most common words from 14011.txt (all words):\n",
      " [(',', 27), ('.', 24), ('he', 12), (':', 10), ('to', 10), (\"'\", 9), ('at', 9), ('George', 9), ('\"', 9), ('a', 8), ('-', 8), ('in', 8), ('and', 8), ('of', 7), ('t', 7), ('1', 6), ('IQ', 6), ('is', 6), ('the', 6), ('his', 6)]\n",
      "20 most common words from 14011.txt (with no stop words):\n",
      " [(',', 27), ('.', 24), (':', 10), (\"'\", 9), ('George', 9), ('\"', 9), ('-', 8), ('1', 6), ('IQ', 6), ('/', 5), ('Yeo', 5), ('.\"', 5), ('old', 4), ('His', 4), ('Mr', 4), ('already', 4), ('Mensa', 3), ('years', 3), ('The', 3), ('When', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here to answer the questions above.\n",
    "\n",
    "# Import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# Define variable for storing stopwords\n",
    "try:\n",
    "    stop_list = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stop_list = stopwords.words('english')\n",
    "# Use file14011 from the previous code and remove the stopwords.\n",
    "# You can store the result in file14011_StopRemove\n",
    "\n",
    "file14011_StopRemove = [w for w in file14011 if w not in stop_list]\n",
    "# Print the new variable with stopwords removed.\n",
    "print(file14011_StopRemove)\n",
    "# Check the top 20 words\n",
    "# Before removing the stop words\n",
    "print(\"20 most common words from 14011.txt (all words):\\n\", nltk.FreqDist(file14011).most_common(20))\n",
    "\n",
    "# After removing them\n",
    "fdist2 = nltk.FreqDist(file14011_StopRemove)\n",
    "\n",
    "print(\"20 most common words from 14011.txt (with no stop words):\\n\", fdist2.most_common(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: You can try the same stop word removal text-preprocessing for entire collection and get the summary of the collection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "NLTK has a built-in Porter stemmer we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer() # creates an instance of the stemmer and assign it to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['singapor', 'expect', 'rain', 'less', 'haze', 'come', 'week', 'south', 'west', 'monsoon', 'season', 'transit', 'inter', 'monsoon', 'condit', 'inter', 'monsoon', 'season', 'typic', 'last', 'octob', 'novemb', 'weather', 'period', 'characteris', 'rainfal', 'light', 'variabl', 'wind', 'meteorolog']\n"
     ]
    }
   ],
   "source": [
    "haze_stemmed = [stemmer.stem(w) for w in haze_stopremoved]\n",
    "print(haze_stemmed[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see from the code above that using the Porter stemmer, “coming” is changed to “come,” “weeks” is changed to “week,” and “transitioning” is changed to “transit.” We can also see that after stemming, some words are no longer correct. For example, “singapore” is changed to “singapor,” “conditions” is changed to “condit,” and so on.</p><p>Although for humans, these words no longer make sense, for computers, this is usually not a problem. As long as all occurrences of “singapore” are changed to “singapor” and all occurrences of “conditions” or “condition” are changed to “condit,” we can still perform many analysis tasks. For example, to search for relevant documents about “singapore,” after stemming, we just need to search for documents containing the word “singapor.”</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "1. Perform stemming on `SGNews_Apr2012`, file `14011.txt` using NLTK’s Porter stemmer. \n",
    "2. Find the most frequent 20 (stemmed) words in this file again.\n",
    "3. Are they very different from your results earlier?\n",
    "4. Apply the same for the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most common words from 14011.txt (with no stemming):\n",
      " [(',', 27), ('.', 24), ('he', 12), (':', 10), ('to', 10), (\"'\", 9), ('at', 9), ('George', 9), ('\"', 9), ('a', 8), ('-', 8), ('in', 8), ('and', 8), ('of', 7), ('t', 7), ('1', 6), ('IQ', 6), ('is', 6), ('the', 6), ('his', 6)]\n",
      "20 most common words from 14011.txt (with stemming):\n",
      " [(',', 27), ('.', 24), ('he', 14), (':', 10), ('at', 10), ('hi', 10), ('to', 10), (\"'\", 9), ('a', 9), ('georg', 9), ('the', 9), ('in', 9), ('\"', 9), ('-', 8), ('and', 8), ('of', 7), ('t', 7), ('1', 6), ('iq', 6), ('is', 6)]\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here to answer the questions above.\n",
    "\n",
    "# Import the porter stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# Define variable to store the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Use file14011 from the previous code and stem the words.\n",
    "# You can store the result in file14011_stem\n",
    "\n",
    "file14011_stem = [stemmer.stem(w) for w in file14011]\n",
    "\n",
    "# Check the top 20 words\n",
    "# Before stemming\n",
    "print(\"20 most common words from 14011.txt (with no stemming):\\n\", nltk.FreqDist(file14011).most_common(20))\n",
    "\n",
    "# After stemming\n",
    "fdist3 = nltk.FreqDist(file14011_stem)\n",
    "print(\"20 most common words from 14011.txt (with stemming):\\n\", fdist3.most_common(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: You can try the same stemming text-preprocessing for entire collection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflective Practice:\n",
    "#### No need to submit.\n",
    "1. Create a Python file in Spyder or VS Code (or any other editor/IDE you like) to perform the same series of steps on a text file, e.g., `mrt.txt`.\n",
    "2. Make the code reusable by keeping the file name as a variable.\n",
    "3. Make it even more reusable by refactoring the code into a function.\n",
    "4. Loop over the files in `SGNews_Apr2012` to perform these tasks.\n",
    "5. Advanced: create appropriate visualizations from your work in the previous step; e.g., word-frequency bar charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "\n",
    "<p>Gensim is another popular Python text analytics library (already installed with default Anaconda installation) that provides some built-in functions for easily converting documents to vectors and computing cosine similarities. Although you can always write your own code to do this, it is much easier for beginners to make use of existing libraries. It is also very common for programmers to re-use libraries developed by other programmers.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Gensim automates common text preprocessing via the simple_preprocess function.</p>\n",
    "\n",
    "Let's try it on the original `haze.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_corpus.raw('haze.txt') = the text read from haze.txt before any processing\n",
    "print( gensim.utils.simple_preprocess( my_corpus.raw('haze.txt') )[:30] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Gensim also has a built-in stop word list for English that can come in handy when we need to remove stop words from a text collection.</p>\n",
    "<p>Practice: compare Gensim's list of stop words with NLTK's.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_list = NLTK's stop words (from above)\n",
    "stop_list_gs = gensim.parsing.preprocessing.STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also has a built-in Porter stemmer we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# usage is the same as NLTK\n",
    "# haze_stemmed = [stemmer.stem(w) for w in haze_stopremoved]\n",
    "# print(haze_stemmed[0:30])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
